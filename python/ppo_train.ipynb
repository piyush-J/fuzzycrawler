{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "headed-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Piyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import scipy.signal\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smoking-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Stub.Stub import StubSession\n",
    "from grammar_lib.testCaseModifier import GCheckModifier\n",
    "from grammar_lib.SQLChecker import parser\n",
    "\n",
    "with open(\"grammar_lib/all_grammar_inputs.txt\") as file:\n",
    "    all_grammar_inputs = [line.strip() for line in file]\n",
    "\n",
    "# all_grammar_inputs_mod = []\n",
    "# for gram_inp in all_grammar_inputs:\n",
    "#     if not any(txt_chk in gram_inp.lower() for txt_chk in ['union', 'order', 'admin', '/*', \"1'=\"]):\n",
    "#         all_grammar_inputs_mod.append(gram_inp)\n",
    "#         all_grammar_inputs_mod.append(gram_inp.replace('OR', 'AND'))\n",
    "#         if random.random()<0.5:\n",
    "#             all_grammar_inputs_mod.append(gram_inp.replace(\"'\", '('))\n",
    "\n",
    "        \n",
    "# union_selected_tests = [\"' ) UNION select NULL,email,pass,NULL from user;\",\n",
    "# \"' ) UNION select NULL,email,pass,NULL from user;\",\n",
    "# \"' UNION select NULL,email,pass,NULL from user;\",\n",
    "# \"' UNION select NULL,email,pass,NULL from user;\",\n",
    "# \"' ) UNION select NULL,email,pass,NULL from user\",\n",
    "# \"' ) UNION select NULL,email,pass,NULL from user\"]\n",
    "\n",
    "# all_grammar_inputs = all_grammar_inputs_mod.copy()#+union_selected_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "postal-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = all_grammar_inputs.copy()\n",
    "\n",
    "sent_processed = []\n",
    "tokenizer = WordPunctTokenizer()\n",
    "max_len = 0\n",
    "\n",
    "for input_str in sent:\n",
    "    input_str = str(input_str)\n",
    "    input_str = input_str.lower()\n",
    "    input_str = input_str.strip()\n",
    "    str_list = nltk.word_tokenize(input_str)\n",
    "    if len(str_list) > max_len:\n",
    "        max_len = len(str_list)\n",
    "    sent_processed.append(str_list)\n",
    "    # sent_processed.append(tokenizer.tokenize(input_str))\n",
    "    \n",
    "all_vocab = [v_w for v in sent_processed for v_w in v]\n",
    "\n",
    "v_count = dict(Counter(all_vocab))\n",
    "v_count = dict(sorted(v_count.items(), key=lambda item: item[1], reverse=True))\n",
    "all_vocab = list(v_count.keys())\n",
    "\n",
    "all_vocab.sort()\n",
    "\n",
    "ind_list = list(range(1, len(all_vocab)+1))\n",
    "\n",
    "word2ind = dict(zip(all_vocab, ind_list))\n",
    "ind2word = dict(zip(ind_list, all_vocab))\n",
    "\n",
    "word2ind['<EOS>'] = 0\n",
    "ind2word[0] = '<EOS>'\n",
    "\n",
    "all_vocab = ['<EOS>']+all_vocab\n",
    "\n",
    "VOCAB_SIZE = len(all_vocab)\n",
    "\n",
    "VOCAB_SIZE, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "departmental-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters of the PPO algorithm\n",
    "MODEL_NAME = \"AttentiveFuzzer\"\n",
    "steps_per_epoch = 50\n",
    "epochs = 5000\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vietnamese-noise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 9 # including EOS\n",
    "\n",
    "loaded_test = [] # store compatible length grammar based init (generation) test strings\n",
    "for sent in sent_processed:\n",
    "    if len(sent)<=MAX_LENGTH-1:\n",
    "        loaded_test.append(sent)\n",
    "        \n",
    "len(loaded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "detected-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eos_and_ind(sampled_list: list, ind: bool = False):\n",
    "    if ind:\n",
    "        eos_token = 0\n",
    "    else:\n",
    "        eos_token = \"<EOS>\"\n",
    "    clip_ind = sampled_list.index(eos_token)\n",
    "    # clip_ind = np.where(sampled_list==eos_token)[0][0] # for numpy\n",
    "    if clip_ind < MAX_LENGTH-1:\n",
    "        clip_ind_rem = MAX_LENGTH-clip_ind\n",
    "        sampled_list = sampled_list[:clip_ind]+[eos_token]*clip_ind_rem # slower than if\n",
    "    assert len(sampled_list) == MAX_LENGTH\n",
    "    if ind:\n",
    "        return sampled_list\n",
    "    return [word2ind[s] for s in sampled_list]\n",
    "\n",
    "def init_string_list():\n",
    "    gram_gen_str = loaded_test[random.randint(0, len(loaded_test)-1)] # randint a<=N<=b\n",
    "    sampled_list = gram_gen_str+(MAX_LENGTH-len(gram_gen_str))*['<EOS>']\n",
    "    # sampled_list = list(random.sample(all_vocab, MAX_LENGTH-1))+['<EOS>']\n",
    "    return eos_and_ind(sampled_list)\n",
    "\n",
    "def mutate_string_list(seed: list, pos: int = None, vocab: int = None):\n",
    "    if vocab is None:\n",
    "        vocab_str = random.sample(all_vocab, 1)[0]\n",
    "        vocab = word2ind[vocab_str]\n",
    "    if pos is None:\n",
    "        pos = random.randint(0, MAX_LENGTH-2) # MAX_LENGTH-2 inclusive\n",
    "    if pos != MAX_LENGTH-1: # should never replace EOS\n",
    "        seed[pos] = vocab\n",
    "    return eos_and_ind(seed, ind=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enormous-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLFuzz:\n",
    "    def __init__(self, rewarding: list = None):\n",
    "        if rewarding is None:\n",
    "            self.init_string = init_string_list() # always as index\n",
    "        else:\n",
    "            self.init_string = rewarding.copy()\n",
    "        self.seed_str = self.init_string.copy()\n",
    "        self.last_str = self.init_string.copy()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\\nOrig seed string: \\n{\" \".join([ind2word[s] for s in self.init_string])}\\nLast seed string: \\n{\" \".join([ind2word[s] for s in self.last_str])}\\nCurr seed string: \\n{\" \".join([ind2word[s] for s in self.seed_str])}'\n",
    "\n",
    "    def action(self, pos, vocab):\n",
    "        self.last_str = self.seed_str.copy()\n",
    "        self.seed_str = mutate_string_list(seed=self.seed_str, pos=pos, vocab=vocab)\n",
    "\n",
    "class RLFuzzEnv:\n",
    "    EXCEPTION_PENALTY = 0.1\n",
    "    MUTATION_PENALTY = 0.2\n",
    "    SAME_STRING_PENALTY = 0.3 # eos & grammar related\n",
    "    PARSER_PENALTY = 0.5\n",
    "    SUCCESS_REWARD = 5\n",
    "    ACTION_SPACE_SIZE_POS = MAX_LENGTH\n",
    "    ACTION_SPACE_SIZE_VOCAB = VOCAB_SIZE\n",
    "\n",
    "    def reset(self, rewarding=None):\n",
    "        self.session = StubSession()\n",
    "        self.last_status = 0\n",
    "        self.fuzzer = RLFuzz(rewarding)\n",
    "        self.episode_step = 0\n",
    "        observation = self.fuzzer.init_string\n",
    "        \n",
    "        self.gmod = GCheckModifier()\n",
    "        self.gparse = parser()\n",
    "        \n",
    "        self.operation = 'None'\n",
    "        \n",
    "        return np.array(observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        action_pos, action_vocab = self.breakdown_action(action)\n",
    "        fuzzing_success = False # init  \n",
    "        self.episode_step += 1\n",
    "        self.fuzzer.action(action_pos, action_vocab)\n",
    "        new_observation = self.fuzzer.seed_str\n",
    "        \n",
    "        eos_index = new_observation.index(0)\n",
    "        new_observation_ = new_observation[:eos_index]\n",
    "        username_rl = \" \".join([ind2word[s] for s in new_observation_])\n",
    "                \n",
    "        eos_indexL = self.fuzzer.last_str.index(0)\n",
    "        new_observationL_ = self.fuzzer.last_str[:eos_indexL]\n",
    "        last_username_rl = \" \".join([ind2word[s] for s in new_observationL_])\n",
    "                \n",
    "        parser_failed = self.gparse.main(self.gmod.grammarchecker(username_rl))\n",
    "        \n",
    "        if self.episode_step>1 and (len(username_rl.strip()) == 0 or last_username_rl==username_rl): # rudimentary\n",
    "            reward = -self.SAME_STRING_PENALTY\n",
    "        elif parser_failed: # parser\n",
    "            reward = -self.PARSER_PENALTY\n",
    "        else: # check via website\n",
    "            if self.last_status == 1:\n",
    "                self.session.reset_session()\n",
    "\n",
    "            url=\"http://localhost/demo/example_mysql_injection_login.php\"\n",
    "            jsonFilePath = './Stub/conditions.json'\n",
    "            receive=self.session.s.get(url)\n",
    "            form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "            values=[username_rl, \"RaNdOmStRiNg\"]\n",
    "            logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "            pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "            status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "            self.last_status = status\n",
    "\n",
    "            fuzzing_success = True if status==1 else False\n",
    "\n",
    "            if fuzzing_success:\n",
    "                reward = self.SUCCESS_REWARD\n",
    "            else:\n",
    "                reward = -self.MUTATION_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if self.episode_step >= steps_per_epoch or (fuzzing_success and self.episode_step>steps_per_epoch//4): #TODO: chk -- removed: @ SUCCESS_REWARD\n",
    "            done = True\n",
    "\n",
    "        return np.array(new_observation), reward, done\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         action_pos, action_vocab = self.breakdown_action(action)\n",
    "#         fuzzing_success = False # init  \n",
    "#         self.episode_step += 1\n",
    "#         self.fuzzer.action(action_pos, action_vocab)\n",
    "#         new_observation = self.fuzzer.seed_str\n",
    "        \n",
    "#         eos_index = new_observation.index(0)\n",
    "#         new_observation_ = new_observation[:eos_index]\n",
    "#         username_rl = \" \".join([ind2word[s] for s in new_observation_])\n",
    "                \n",
    "#         eos_indexL = self.fuzzer.last_str.index(0)\n",
    "#         new_observationL_ = self.fuzzer.last_str[:eos_indexL]\n",
    "#         last_username_rl = \" \".join([ind2word[s] for s in new_observationL_])\n",
    "                \n",
    "#         parser_failed = self.gparse.main(self.gmod.grammarchecker(username_rl))\n",
    "        \n",
    "#         if self.episode_step>1 and (len(username_rl.strip()) == 0 or last_username_rl==username_rl): # rudimentary\n",
    "#             # print(f\"SAME_STRING_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "#             reward = -self.SAME_STRING_PENALTY\n",
    "#         elif parser_failed: # parser\n",
    "#             # print(f\"PARSER_PENALTY @ {self.episode_step}: \", username_rl)\n",
    "#             reward = -self.PARSER_PENALTY\n",
    "#         else: # check via website\n",
    "#             if self.last_status == 1:\n",
    "#                 self.session.reset_session()\n",
    "            \n",
    "#             # Search box\n",
    "\n",
    "#             url=\"http://localhost/demo/example_mysql_injection_search_box.php\"\n",
    "#             jsonFilePath = './Stub/conditions1.json'\n",
    "#             receive=self.session.s.get(url)\n",
    "#             form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "#             values=[username_rl]\n",
    "#             logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "#             pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "#             status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "            \n",
    "#             status_ex = self.session.exceptionCatcher(url, logindata)\n",
    "            \n",
    "#             exception_success = True if status_ex==1 else False\n",
    "            \n",
    "#             fuzzing_success1 = True if status==1 else False\n",
    "            \n",
    "#             # Login page\n",
    "            \n",
    "#             url=\"http://localhost/demo/example_mysql_injection_login.php\"\n",
    "#             jsonFilePath = './Stub/conditions.json'\n",
    "#             receive=self.session.s.get(url)\n",
    "#             form_details,keys=self.session.preprocessing_Form_Fields(url)\n",
    "\n",
    "#             values=[username_rl, \"RaNdOmStRiNg\"]\n",
    "#             logindata=self.session.form_input_feeding(keys,values,form_details)\n",
    "#             pass_Conditions, fail_Conditions = self.session.jsonReading(jsonFilePath)\n",
    "#             status = self.session.validation(url, logindata, keys, pass_Conditions, fail_Conditions)\n",
    "#             self.last_status = status\n",
    "#             fuzzing_success2 = True if status==1 else False\n",
    "            \n",
    "#             # Common\n",
    "\n",
    "#             fuzzing_success = fuzzing_success1 or fuzzing_success2\n",
    "            \n",
    "#             if fuzzing_success:\n",
    "#                 reward = self.SUCCESS_REWARD\n",
    "#             elif exception_success:\n",
    "#                 reward = -self.EXCEPTION_PENALTY\n",
    "#             else:\n",
    "#                 reward = -self.MUTATION_PENALTY\n",
    "\n",
    "#         done = False\n",
    "#         if self.episode_step >= steps_per_epoch or (fuzzing_success and self.episode_step>steps_per_epoch//4): #TODO: chk -- removed: @ SUCCESS_REWARD\n",
    "#             done = True\n",
    "\n",
    "#         return np.array(new_observation), reward, done\n",
    "    \n",
    "    def breakdown_action(self, action):\n",
    "        action_pos = action//VOCAB_SIZE\n",
    "        action_vocab = action%VOCAB_SIZE\n",
    "        # print('POS: ', action_pos, 'VOCAB: ', action_vocab, 'CONV: ', action_pos*VOCAB_SIZE+action_vocab)\n",
    "        return action_pos, action_vocab\n",
    "\n",
    "    def squeeze_actions(self, action_pos, action_vocab):\n",
    "        return action_pos*VOCAB_SIZE+action_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comparable-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerBlock(layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.ffn = keras.Sequential(\n",
    "#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = layers.Dropout(rate)\n",
    "#         self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "#     def call(self, inputs, training):\n",
    "#         attn_output, weights = self.att(inputs, inputs, return_attention_scores=True)\n",
    "#         attn_output1 = self.dropout1(attn_output, training=training)\n",
    "#         out1 = self.layernorm1(inputs + attn_output1)\n",
    "#         ffn_output = self.ffn(out1)\n",
    "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
    "#         return self.layernorm2(out1 + ffn_output), weights\n",
    "    \n",
    "# class TokenAndPositionEmbedding(layers.Layer):\n",
    "#     def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "#         super(TokenAndPositionEmbedding, self).__init__()\n",
    "#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         maxlen = tf.shape(x)[-1]\n",
    "#         positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "#         positions = self.pos_emb(positions)\n",
    "#         x = self.token_emb(x)\n",
    "#         return x + positions\n",
    "    \n",
    "# def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "#     embed_dim = 32  # Embedding size for each token\n",
    "#     num_heads = 4  # Number of attention heads\n",
    "#     ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "#     embedding_layer = TokenAndPositionEmbedding(MAX_LENGTH, VOCAB_SIZE, embed_dim)\n",
    "#     x = embedding_layer(x)\n",
    "#     transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "#     x, attn_output = transformer_block(x)\n",
    "#     x = layers.GlobalAveragePooling1D()(x)\n",
    "#     x = layers.Dropout(0.1)(x)\n",
    "#     for size in sizes[:-1]:\n",
    "#         x = layers.Dense(units=size, activation=activation)(x)\n",
    "#     return layers.Dense(units=sizes[-1], activation=output_activation)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expanded-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    x = layers.Embedding(VOCAB_SIZE, 16)(x)\n",
    "    x= layers.GRU(32, return_sequences=False)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "western-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hindu-insert",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions:  414 9*46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 1/5000 [00:11<15:59:33, 11.52s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1. Mean Return: -23.40. Mean Length: 50.00. Success Count: 0\n",
      "Mean Aggregate:  -23.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                        | 2/5000 [00:17<13:49:25,  9.96s/episodes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 2. Mean Return: -19.30. Mean Length: 50.00. Success Count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 2/5000 [00:18<12:44:48,  9.18s/episodes]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d8dc9717b548>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mobs_text_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mobservation_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msuccess_count\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mepisode_return\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-38b1c73ed885>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mlogindata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mform_input_feeding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mform_details\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mpass_Conditions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfail_Conditions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjsonReading\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsonFilePath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpass_Conditions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfail_Conditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\fuzzycrawler\\python\\Stub\\Stub.py\u001b[0m in \u001b[0;36mvalidation\u001b[1;34m(self, url, logindata, keys, pass_Conditions, fail_Conditions)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mpcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPFValidations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpass_Conditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[0mfcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPFValidations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfail_Conditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\fuzzycrawler\\python\\Stub\\Stub.py\u001b[0m in \u001b[0;36mPFValidations\u001b[1;34m(self, url, logindata, keys, Conditions)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mPFValidations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mreceive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0msend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogindata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0mfinalStatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \"\"\"\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UW_AI\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Initializations\n",
    "\"\"\"\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "agg_rewards = []\n",
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = RLFuzzEnv()\n",
    "observation_dimensions = MAX_LENGTH\n",
    "num_actions = MAX_LENGTH*VOCAB_SIZE\n",
    "print('num_actions: ', num_actions, f'{MAX_LENGTH}*{VOCAB_SIZE}')\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "\"\"\"\n",
    "## Train\n",
    "\"\"\"\n",
    "# Iterate over the number of epochs\n",
    "for epoch in tqdm(range(epochs), ascii=True, unit='episodes'):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    \n",
    "    success_count = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        \n",
    "        obs_text_list = [ind2word[i] for i in observation[0]]\n",
    "            \n",
    "        observation_new, reward, done = env.step(action[0].numpy())\n",
    "        if reward > 0: success_count+=1\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    agg_rewards.append(round((sum_return / num_episodes), 2))\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {(sum_return / num_episodes):.2f}. Mean Length: {(sum_length / num_episodes):.2f}. Success Count: {success_count}\"\n",
    "    )\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        mean_agg = sum(agg_rewards[-100:])/len(agg_rewards[-100:])\n",
    "        print(\"Mean Aggregate: \", mean_agg)\n",
    "        #actor.save(f'models/{MODEL_NAME}__ep_{epoch}__{(sum_return / num_episodes):.2f}__actor')\n",
    "        #critic.save(f'models/{MODEL_NAME}__ep_{epoch}__{(sum_return / num_episodes):.2f}__critic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor.save(f'models/{MODEL_NAME}_FINAL__ep_{epoch}__{(sum_return / num_episodes):.2f}__actor')\n",
    "#critic.save(f'models/{MODEL_NAME}_FINAL__ep_{epoch}__{(sum_return / num_episodes):.2f}__critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-assist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
